{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9160d5",
   "metadata": {},
   "source": [
    "# MLOps - Real Time Indonesia Sentiment Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1 : Problem\n",
    "\n",
    "Di tahap ini kita akan berfokus pada problem yang akan kita selesaikan, apa yang menjadi landasan dari sistem ini dibuat. apa saja kendala yang dialami sekarang pada sektor ini dan bagaimana sistem bisa membantu menyelesaikan nya.\n",
    "\n",
    "### 1.1 Business Problem\n",
    "\n",
    "Coba kita pikir, gimana sentimen bisa mempengaruhi nilai bisnis suatu perusahaan??. Sebenarnya as simple as komen yang ga bener dan jatuhin reputasi nya, kan banyak banget tu kejadian di netizen indo. aplikasi ngebug dikit langsung dikatain, dan yang liat itu langsung berpendapat yang sama, sehingga valuasi bisnis di mata publik menurun. \n",
    "\n",
    "So, gimana sistem gitu ngasih solusi untuk masalah ini??. \n",
    "\n",
    "Sistem akan bantu mendeteksi dan menemukan anomali ini, ketika ada komentar negatif, perusahaan akan tahu dimana saja komentarnya. Intinya dengan begini, mereka bisa tau siapa saja yang harus ditindaklanjuti, yaa bisa dengan dengerin keluhannya, dan berdamai gitu. atau just, laporin kalo emang ga jelas.\n",
    "\n",
    "Ga jauh dari masalah komen netizen, masih di tema yang sama yaitu review. yaa, apa lagi kalo bukan review ecommerce. Kalo banyak yang review product, dan penjual pengen tau review negatif yang harus ditindaklanjuti, mereka bakal keteteran. Makanya dengan sistem ini, review negatif dan positif bisa tersortir. Sistem lebih ke babu yang disuruh buat kupasin kuaci.\n",
    "\n",
    "Terakhir selain di ranah komersil. Sebenarnya sistem ini bisa bantu banget dalam penyebaran informasi yang lebih baik dan anti hoax. Tapi, itu kan urusan si penyedia layanan sosial media, twitter kasih, ig ga kasih.. So ini lebih ke balik lagi ke diri sendiri atau organisasi yang pengen atur gimana informasi mereka bisa tersebar dengan baik. Misal during kampanye, tokoh politik bisa pake sistem ini buat nyari komen negatif soal mereka dan langsung blokir itu komen, supaya reputasi mereka tetep baik di mata publik. Ga cuman sampe situ. artis atau pengguna sosmed bisa lakukan hal yang sama, mereka bisa track gimana informasi tentang mereka beredar, dan mereka bisa lakukan keputusan tertentu berdesarkan itu nantinya.\n",
    "\n",
    "So overall bisnis problemnya adalah perusahaan/firm, ecommerce seller, dan organisasi publik, dan selebriti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a242dc6",
   "metadata": {},
   "source": [
    "### 1.2. Stakeholders and Metrics\n",
    "\n",
    "Untuk menjamin keberhasilan projek, kita harus tau siapa saja yang perlu sistem dan bagaimana mengukur keberhasilannya. \n",
    "\n",
    "- Stake holder : Ecommerce seller, pen tau bisnis/product performance dari review. \n",
    "- PR, pantau reputasi produk atau kampanye di mata publikk. \n",
    "- Tim customer support, pen tau dimana masalah nya dan cepet selesain. \n",
    "- Scientis, pake data atau model untuk analisis lebih dalam. \n",
    "\n",
    "Success Metrics : \n",
    "\n",
    "- MTTD (Mean Time to Detect), yaitu waktu rata rata untuk melihat lonjakan sentimen negatif. Target < 15 menit. Menyelesaikan malasah keluhan berkurang 25%\n",
    "- Model Performance, akurasi tinggi, F1 Score > 0.85. Tapi gabias, makanya Recall kelas negatif tinggi > .90. supaya ga kelewat kelas negatif.\n",
    "- SLO Operational, Latensi <200ms, Throughput 100 request/s. Uptime 24/7.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efb7d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2 : Dataset\n",
    "\n",
    "### 2.1. Fragmanted Data\n",
    "\n",
    "Data bahasa indonesia memang dinilai merupakan sumber daya rendah untuk melatih machine learning. Tapi sebanrnya bukan karena kurang atau tidak ada. tapi masalah utamanya terletak di fragmantasi data. data terkumpul tidak terstruktur, tercampur dari berbagai sumber, wikipedia, medsos, berita dll, sehingga skema data tidak sama. Oleh karena itu tugas kita di feature engeneering nanti adalah bisa menggabungkan banyak sumber data yang berbeda beda ini menjadi satu corpus yang komprehensif dan kaya akan informasi yang diperlukan.\n",
    "\n",
    "### 2.2. Source\n",
    "\n",
    "Kita akan menggunakan beberapa dataset publik seperti :\n",
    "\n",
    "`indonlp/NusaX-Senti` -> Dataset publik yang punya 3 kelas netral positif dan negatif, dengan data yang sudah cukup bersih, dan mencakup beberapa bahasa daerah untuk perkaya konteks.\n",
    "\n",
    "`indonlp/indonlu` -> subset dari `smsa`, bagian dari benchmark `IndoNLU`, dataset ini berisi komentar dan ulasan dari berbegai layanan internet, dan dianotasikan oleh profesional. Model seperti `taufiqdp/indonesian-sentiment` sangat terkenal digunakan pada data ini dan sudah memvalidasi jika data bagus sekali untuk latih model itu.\n",
    "\n",
    "Kaggle -> kaggle merupakan sumber dataset terakhir yang mana punya data sentiment yang banyak juga, walau kadang kurang bersih atau valid. `jocelyndumlao/prdect-id` dan `alvinhanafie/dataset-for-indonesian-sentiment-analysis` akan menjadi pilihan dataset untuk projek ini dari kaggle.\n",
    "\n",
    "Overall, dataset kita kira kira gini..\n",
    "\n",
    "| Nama Dataset                                             | Sumber       | Ukuran               | Label                               | Domain                        | Lisensi            |\n",
    "| -------------------------------------------------------- | ------------ | -------------------- | ----------------------------------- | ----------------------------- | ------------------ |\n",
    "| `indonlp/NusaX-senti`                                    | Hugging Face | ~12k baris           | Positif, Netral, Negatif            | Umum/Berita                   | CC BY-SA 4.0       |\n",
    "| `indonlp/indonlu` (smsa)                                 | Hugging Face | ~11k train, ~1k test | Positif, Netral, Negatif            | Ulasan Umum                   | Tidak Ditentukan   |\n",
    "| `jocelyndumlao/prdect-id`                                | Kaggle       | ~7.7k baris          | Emosi (dapat dipetakan ke sentimen) | Ulasan E-commerce (Tokopedia) | CC0: Public Domain |\n",
    "| `alvinhanafie/dataset-for-indonesian-sentiment-analysis` | Kaggle       | ~11k baris           | Positif, Netral, Negatif            | Umum                          | Tidak Ditentukan   |\n",
    "\n",
    "So, yang pen kita lakukan adalah mengunduh atau mencari semua dataset di atas, membersihkannya, kemudian menggabungkan nya menjadi lebih terstruktur, terstandarisasi, normalisasi, terskala dan terprogram serta siap untuk model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978d012",
   "metadata": {},
   "source": [
    "### 1.3. Data Preparation - Indonesian Language Problem\n",
    "\n",
    "Ada beberapa yang jadi case spesial disini. Karena teks pada media sosial yang digunakan oleh masyarakat indonesia itu sering kali tidak formal, sering menggunakan ejaan yang salah, pake singkatan dan campur aduk sama bahasa asing kaya inggris misalnya. So pipeline preprocessing kita harus bisa handle ini, dengan apa??\n",
    "\n",
    "Nah, caranya tergantung dari model yang mau dipake, kalo kita pake TF-IDF, maka praprocessing harus menyesuaikan dengan model yaitu secara agressif membersihkan data dengan stemming. Sedangkan untuk model BERT, mereka sudah memiliki tokenizer nya sendiri yang bisa konversi data mentah menjadi informasi yang dilihat dari referensi yang sudah dipelajari sebelumnya. Intinya bert udah belajar duluan, jadi saat ada bebrapa kata baru kaya singkatan dll, dia bisa metain itu dan cocokin dan pahamin lagi.\n",
    "\n",
    "#### Pipeline 1 : TF-IDF (Term Frequency - Inverse Document Frequenncy)\n",
    "\n",
    "Tf IDF basically mainin frekuensi dari suatu kata, dengan melihat frekuensi ini dia bisa nentuin seberapa penting suatu kata di dalam korpus, dan untuk semua kata. dari situ dia belajar konteks atau makna dari suatu kata dengan kata lain.\n",
    "\n",
    "Step by step : \n",
    "\n",
    "1. Case folding : Kecilin semua huruf\n",
    "\n",
    "2. Normalisasi : pake kamus alay dari publik untuk konversi singkatan dan juga emoji\n",
    "\n",
    "3. Cleaning : hapus url, non case / non alfanumerik, serta hastag, stopword (ga punya makna penting).\n",
    "\n",
    "4. Stemming : pake PySastrawi buat hapus imbuhan, menggunakan -> gunakan.\n",
    "\n",
    "#### Pipeline 2 : IndoBERT (Indo Bidirectional Encoder Representation of Transformer)\n",
    "\n",
    "BERT itu transformer, transformer pada dasarnya bekerja dengan metain kata kata di bidang n dimensi, kalo ada kata makna nya deket ya koordinatnya deket. itu aja sih.\n",
    "\n",
    "Step by step :\n",
    "\n",
    "1. Case folding, norm, cleaning ..\n",
    "\n",
    "2. Tokenizing (stopword dan imbuhan dibiarin lewat karena bisa kasih makna lebih)\n",
    "\n",
    "Terakhir untuk code-switcing problem, atau bahasa yang kecampur, solusinya adalah pake `indobenchmark/indobert-base-p1`. Ini model udah dilatih dengan data textual yang kaya informasi dengan size 23gban, dan di datanya udah ada multibahasa juga, jadi konteksnya udah dapet lah ya.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836845de",
   "metadata": {},
   "source": [
    "### 2.4. Data Preprocessing - Class Imbalance\n",
    "\n",
    "Yaa ini udah umum juga terjadi, data yang ga balance itu intinya cuman satu kelas misalnya sentimen negatif itu terlalu mayoritas. sehingga oversampling data, masalahnya timbul kalo kita training model, model bakal berat sebelah, karena kebanyakan data sentiment negatif, dia juga belajar untuk labellin lebih ke sentimen negatif juga. apa apa nanti sentiment negatif jatuhnya. Solusinya kalo di NLP itu pake:\n",
    "\n",
    "**Data Augmentation :** Imputasi, upsampling kelas minoritas, caranya pake \n",
    "\n",
    "- Back-Translation, teks bindo diterjemahi ke inggris bistu diterjemahin lagi ke indo, nambah variasi, lebih ke sinonimnya lah, walau makna sama tapi kata katanya beda. \n",
    "- npaug, pustaka ini bisa ganti kata dengan sinonim, dan juga hapus dan nambah kata secara acak.\n",
    "\n",
    "**Training-Time Techiques :** \n",
    "\n",
    "- Class Weight : model dihukum lebih kelas kalau salah prediksi kelas minoritas, jadi ga lupa sama kelas minoritas itu.\n",
    "- Focal Loss : sama kaya class weight, tapi fokusnya ke contoh kata yang susah ditebak, jadi model ga cuman belajar dari contoh yang gampang, lebih ke pemilihan sample aja sih."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045d316",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
