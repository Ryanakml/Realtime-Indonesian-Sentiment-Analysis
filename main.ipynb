{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9160d5",
   "metadata": {},
   "source": [
    "# MLOps - Real Time Indonesia Sentiment Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1 : Problem\n",
    "\n",
    "Di tahap ini kita akan berfokus pada problem yang akan kita selesaikan, apa yang menjadi landasan dari sistem ini dibuat. apa saja kendala yang dialami sekarang pada sektor ini dan bagaimana sistem bisa membantu menyelesaikan nya.\n",
    "\n",
    "### 1.1 Business Problem\n",
    "\n",
    "Coba kita pikir, gimana sentimen bisa mempengaruhi nilai bisnis suatu perusahaan??. Sebenarnya as simple as komen yang ga bener dan jatuhin reputasi nya, kan banyak banget tu kejadian di netizen indo. aplikasi ngebug dikit langsung dikatain, dan yang liat itu langsung berpendapat yang sama, sehingga valuasi bisnis di mata publik menurun. \n",
    "\n",
    "So, gimana sistem gitu ngasih solusi untuk masalah ini??. \n",
    "\n",
    "Sistem akan bantu mendeteksi dan menemukan anomali ini, ketika ada komentar negatif, perusahaan akan tahu dimana saja komentarnya. Intinya dengan begini, mereka bisa tau siapa saja yang harus ditindaklanjuti, yaa bisa dengan dengerin keluhannya, dan berdamai gitu. atau just, laporin kalo emang ga jelas.\n",
    "\n",
    "Ga jauh dari masalah komen netizen, masih di tema yang sama yaitu review. yaa, apa lagi kalo bukan review ecommerce. Kalo banyak yang review product, dan penjual pengen tau review negatif yang harus ditindaklanjuti, mereka bakal keteteran. Makanya dengan sistem ini, review negatif dan positif bisa tersortir. Sistem lebih ke babu yang disuruh buat kupasin kuaci.\n",
    "\n",
    "Terakhir selain di ranah komersil. Sebenarnya sistem ini bisa bantu banget dalam penyebaran informasi yang lebih baik dan anti hoax. Tapi, itu kan urusan si penyedia layanan sosial media, twitter kasih, ig ga kasih.. So ini lebih ke balik lagi ke diri sendiri atau organisasi yang pengen atur gimana informasi mereka bisa tersebar dengan baik. Misal during kampanye, tokoh politik bisa pake sistem ini buat nyari komen negatif soal mereka dan langsung blokir itu komen, supaya reputasi mereka tetep baik di mata publik. Ga cuman sampe situ. artis atau pengguna sosmed bisa lakukan hal yang sama, mereka bisa track gimana informasi tentang mereka beredar, dan mereka bisa lakukan keputusan tertentu berdesarkan itu nantinya.\n",
    "\n",
    "So overall bisnis problemnya adalah perusahaan/firm, ecommerce seller, dan organisasi publik, dan selebriti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a242dc6",
   "metadata": {},
   "source": [
    "### 1.2. Stakeholders and Metrics\n",
    "\n",
    "Untuk menjamin keberhasilan projek, kita harus tau siapa saja yang perlu sistem dan bagaimana mengukur keberhasilannya. \n",
    "\n",
    "- Stake holder : Ecommerce seller, pen tau bisnis/product performance dari review. \n",
    "- PR, pantau reputasi produk atau kampanye di mata publikk. \n",
    "- Tim customer support, pen tau dimana masalah nya dan cepet selesain. \n",
    "- Scientis, pake data atau model untuk analisis lebih dalam. \n",
    "\n",
    "Success Metrics : \n",
    "\n",
    "- MTTD (Mean Time to Detect), yaitu waktu rata rata untuk melihat lonjakan sentimen negatif. Target < 15 menit. Menyelesaikan malasah keluhan berkurang 25%\n",
    "- Model Performance, akurasi tinggi, F1 Score > 0.85. Tapi gabias, makanya Recall kelas negatif tinggi > .90. supaya ga kelewat kelas negatif.\n",
    "- SLO Operational, Latensi <200ms, Throughput 100 request/s. Uptime 24/7.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efb7d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2 : Dataset\n",
    "\n",
    "### 2.1. Fragmanted Data\n",
    "\n",
    "Data bahasa indonesia memang dinilai merupakan sumber daya rendah untuk melatih machine learning. Tapi sebanrnya bukan karena kurang atau tidak ada. tapi masalah utamanya terletak di fragmantasi data. data terkumpul tidak terstruktur, tercampur dari berbagai sumber, wikipedia, medsos, berita dll, sehingga skema data tidak sama. Oleh karena itu tugas kita di feature engeneering nanti adalah bisa menggabungkan banyak sumber data yang berbeda beda ini menjadi satu corpus yang komprehensif dan kaya akan informasi yang diperlukan.\n",
    "\n",
    "### 2.2. Source\n",
    "\n",
    "Kita akan menggunakan beberapa dataset publik seperti :\n",
    "\n",
    "`indonlp/NusaX-Senti` -> Dataset publik yang punya 3 kelas netral positif dan negatif, dengan data yang sudah cukup bersih, dan mencakup beberapa bahasa daerah untuk perkaya konteks.\n",
    "\n",
    "`indonlp/indonlu` -> subset dari `smsa`, bagian dari benchmark `IndoNLU`, dataset ini berisi komentar dan ulasan dari berbegai layanan internet, dan dianotasikan oleh profesional. Model seperti `taufiqdp/indonesian-sentiment` sangat terkenal digunakan pada data ini dan sudah memvalidasi jika data bagus sekali untuk latih model itu.\n",
    "\n",
    "Kaggle -> kaggle merupakan sumber dataset terakhir yang mana punya data sentiment yang banyak juga, walau kadang kurang bersih atau valid. `jocelyndumlao/prdect-id` dan `alvinhanafie/dataset-for-indonesian-sentiment-analysis` akan menjadi pilihan dataset untuk projek ini dari kaggle.\n",
    "\n",
    "Overall, dataset kita kira kira gini..\n",
    "\n",
    "| Nama Dataset                                             | Sumber       | Ukuran               | Label                               | Domain                        | Lisensi            |\n",
    "| -------------------------------------------------------- | ------------ | -------------------- | ----------------------------------- | ----------------------------- | ------------------ |\n",
    "| `indonlp/NusaX-senti`                                    | Hugging Face | ~12k baris           | Positif, Netral, Negatif            | Umum/Berita                   | CC BY-SA 4.0       |\n",
    "| `indonlp/indonlu` (smsa)                                 | Hugging Face | ~11k train, ~1k test | Positif, Netral, Negatif            | Ulasan Umum                   | Tidak Ditentukan   |\n",
    "| `jocelyndumlao/prdect-id`                                | Kaggle       | ~7.7k baris          | Emosi (dapat dipetakan ke sentimen) | Ulasan E-commerce (Tokopedia) | CC0: Public Domain |\n",
    "| `alvinhanafie/dataset-for-indonesian-sentiment-analysis` | Kaggle       | ~11k baris           | Positif, Netral, Negatif            | Umum                          | Tidak Ditentukan   |\n",
    "\n",
    "So, yang pen kita lakukan adalah mengunduh atau mencari semua dataset di atas, membersihkannya, kemudian menggabungkan nya menjadi lebih terstruktur, terstandarisasi, normalisasi, terskala dan terprogram serta siap untuk model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8274bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11433, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  warung ini dimiliki oleh pengusaha pabrik tahu...      2\n",
       "1  mohon ulama lurus dan k212 mmbri hujjah partai...      1\n",
       "2  lokasi strategis di jalan sumatera bandung . t...      2\n",
       "3  betapa bahagia nya diri ini saat unboxing pake...      2\n",
       "4  duh . jadi mahasiswa jangan sombong dong . kas...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset parquet\n",
    "df = pd.read_parquet(\"data/raw/unified_sentiment_corpus.parquet\")\n",
    "# Cek dimensi\n",
    "print(df.shape)\n",
    "# Lihat 5 baris pertama\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978d012",
   "metadata": {},
   "source": [
    "### 1.3. Data Preparation - Indonesian Language Problem\n",
    "\n",
    "Ada beberapa yang jadi case spesial disini. Karena teks pada media sosial yang digunakan oleh masyarakat indonesia itu sering kali tidak formal, sering menggunakan ejaan yang salah, pake singkatan dan campur aduk sama bahasa asing kaya inggris misalnya. So pipeline preprocessing kita harus bisa handle ini, dengan apa??\n",
    "\n",
    "Nah, caranya tergantung dari model yang mau dipake, kalo kita pake TF-IDF, maka praprocessing harus menyesuaikan dengan model yaitu secara agressif membersihkan data dengan stemming. Sedangkan untuk model BERT, mereka sudah memiliki tokenizer nya sendiri yang bisa konversi data mentah menjadi informasi yang dilihat dari referensi yang sudah dipelajari sebelumnya. Intinya bert udah belajar duluan, jadi saat ada bebrapa kata baru kaya singkatan dll, dia bisa metain itu dan cocokin dan pahamin lagi.\n",
    "\n",
    "#### Pipeline 1 : TF-IDF (Term Frequency - Inverse Document Frequenncy)\n",
    "\n",
    "Tf IDF basically mainin frekuensi dari suatu kata, dengan melihat frekuensi ini dia bisa nentuin seberapa penting suatu kata di dalam korpus, dan untuk semua kata. dari situ dia belajar konteks atau makna dari suatu kata dengan kata lain.\n",
    "\n",
    "Step by step : \n",
    "\n",
    "1. Case folding : Kecilin semua huruf\n",
    "\n",
    "2. Normalisasi : pake kamus alay dari publik untuk konversi singkatan dan juga emoji\n",
    "\n",
    "3. Cleaning : hapus url, non case / non alfanumerik, serta hastag, stopword (ga punya makna penting).\n",
    "\n",
    "4. Stemming : pake PySastrawi buat hapus imbuhan, menggunakan -> gunakan.\n",
    "\n",
    "#### Pipeline 2 : IndoBERT (Indo Bidirectional Encoder Representation of Transformer)\n",
    "\n",
    "BERT itu transformer, transformer pada dasarnya bekerja dengan metain kata kata di bidang n dimensi, kalo ada kata makna nya deket ya koordinatnya deket. itu aja sih.\n",
    "\n",
    "Step by step :\n",
    "\n",
    "1. Case folding, norm, cleaning ..\n",
    "\n",
    "2. Tokenizing (stopword dan imbuhan dibiarin lewat karena bisa kasih makna lebih)\n",
    "\n",
    "Terakhir untuk code-switcing problem, atau bahasa yang kecampur, solusinya adalah pake `indobenchmark/indobert-base-p1`. Ini model udah dilatih dengan data textual yang kaya informasi dengan size 23gban, dan di datanya udah ada multibahasa juga, jadi konteksnya udah dapet lah ya.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836845de",
   "metadata": {},
   "source": [
    "### 2.4. Data Preprocessing - Class Imbalance\n",
    "\n",
    "Yaa ini udah umum juga terjadi, data yang ga balance itu intinya cuman satu kelas misalnya sentimen negatif itu terlalu mayoritas. sehingga oversampling data, masalahnya timbul kalo kita training model, model bakal berat sebelah, karena kebanyakan data sentiment negatif, dia juga belajar untuk labellin lebih ke sentimen negatif juga. apa apa nanti sentiment negatif jatuhnya. Solusinya kalo di NLP itu pake:\n",
    "\n",
    "**Data Augmentation :** Imputasi, upsampling kelas minoritas, caranya pake \n",
    "\n",
    "- Back-Translation, teks bindo diterjemahi ke inggris bistu diterjemahin lagi ke indo, nambah variasi, lebih ke sinonimnya lah, walau makna sama tapi kata katanya beda. \n",
    "- npaug, pustaka ini bisa ganti kata dengan sinonim, dan juga hapus dan nambah kata secara acak.\n",
    "\n",
    "**Training-Time Techiques :** \n",
    "\n",
    "- Class Weight : model dihukum lebih kelas kalau salah prediksi kelas minoritas, jadi ga lupa sama kelas minoritas itu.\n",
    "- Focal Loss : sama kaya class weight, tapi fokusnya ke contoh kata yang susah ditebak, jadi model ga cuman belajar dari contoh yang gampang, lebih ke pemilihan sample aja sih."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045d316",
   "metadata": {},
   "source": [
    "## Section 3 : Data Pipeline and ETL\n",
    "\n",
    "### 3.1. Ingestion Machine : Static Data and Streaming\n",
    "\n",
    "Sistem harus bisa dua jenis data :\n",
    "\n",
    "#### Data Statis (Batch) \n",
    "\n",
    "Data lama atau historis, contoh dataset yang kita extract dari hugging face atau kaggle, transform dan cleaning it, kemudian digabungin ke dalam satu file Parquet. Proses ini akan dilakukan pada skript [`make_dataset.py`](src/data/make_dataset.py). \n",
    "\n",
    "Cara Menjalankannya :\n",
    "\n",
    "```bash\n",
    "# pastikan kita berada di root project direktori\n",
    "mkdir -p data/raw\n",
    "python src/data/make_dataset.py\n",
    "```\n",
    "\n",
    "#### Data Streaming\n",
    "\n",
    "Data baru yang datang dari media sosial atau input realtime dari user.\n",
    "\n",
    "Karena kita butuh data real-time, tapi nggak selalu bisa langsung pakai API resmi (misalnya Twitter/X atau Reddit), maka dibuat simulator:\n",
    "Kenapa simulator? -> Menghindari masalah hukum/ToS (scraping bisa melanggar aturan) dan biar kita tetap bisa uji coba sistem real-time meskipun nggak punya akses API asli.\n",
    "\n",
    "Skrip [`stream_simulator.py`](src/data/stream_simulator.py) akan bikin data palsu (dummy) dalam format JSON, mirip banget dengan data asli dari Twitter/X API v2 atau Reddit API. Kalau nanti dapat API resmi, tinggal ganti sumber datanya, sistem tetap jalan.\n",
    "\n",
    "Cara Menjalankannya :\n",
    "\n",
    "```bash\n",
    "# Pastikan Kafka dan Zookeeper berjalan (misalnya, via docker-compose)\n",
    "# Jalankan simulator untuk Twitter\n",
    "python src/data/stream_simulator.py --source twitter\n",
    "\n",
    "# Di terminal lain, jalankan simulator untuk Reddit\n",
    "python src/data/stream_simulator.py --source reddit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8988cb58",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found NusaX-senti.py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindonlp/NusaX-senti\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/[1] BS/[2] Freelance/[3] PROJECTS/MLOPS/Real-time Indonesian Sentiment Analysis/myvenv/lib/python3.10/site-packages/datasets/load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1389\u001b[0m )\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/Documents/[1] BS/[2] Freelance/[3] PROJECTS/MLOPS/Real-time Indonesian Sentiment Analysis/myvenv/lib/python3.10/site-packages/datasets/load.py:1132\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[0;32m-> 1132\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/Documents/[1] BS/[2] Freelance/[3] PROJECTS/MLOPS/Real-time Indonesian Sentiment Analysis/myvenv/lib/python3.10/site-packages/datasets/load.py:1031\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1027\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1028\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1031\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/[1] BS/[2] Freelance/[3] PROJECTS/MLOPS/Real-time Indonesian Sentiment Analysis/myvenv/lib/python3.10/site-packages/datasets/load.py:989\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m    983\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    984\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    988\u001b[0m     )\n\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found NusaX-senti.py"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"indonlp/NusaX-senti\", \"ind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4593b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e703eab8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
